{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a58c766",
   "metadata": {},
   "source": [
    "# Depression Text Analysis: Word Frequency & Correlation Study\n",
    "\n",
    "## Project Overview\n",
    "This notebook analyzes linguistic patterns in psychologist-patient conversations to understand correlations between word usage and depression levels.\n",
    "\n",
    "**Approach (following professor's recommendations):**\n",
    "1. Start with simple word frequency distributions\n",
    "2. Analyze correlations between word frequencies and depression levels  \n",
    "3. Progressively move to advanced text processing techniques\n",
    "\n",
    "**Dataset:** Interview transcripts from AVEC 2017 Depression Recognition Challenge\n",
    "- Raw transcripts in `data/raw/`\n",
    "- Depression labels (PHQ scores) in `data/labels/processed/`\n",
    "- ~190 participants with binary depression classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8913d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# Import our custom modules\n",
    "from data_loader import DataLoader\n",
    "from text_preprocessing import TextPreprocessor, CustomStopwords\n",
    "from frequency_analysis import WordFrequencyAnalyzer, CorrelationAnalyzer\n",
    "from visualization import DataVisualizer\n",
    "import config\n",
    "\n",
    "print(\"✓ All libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133a1098",
   "metadata": {},
   "source": [
    "## Section 1: Load and Explore Data\n",
    "\n",
    "Let's start by loading the raw transcripts and depression labels to understand our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308896e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "data_loader = DataLoader('../data')\n",
    "\n",
    "# Load labels\n",
    "labels = data_loader.load_labels(use_processed=True)\n",
    "print(\"Depression Labels Dataset:\")\n",
    "print(f\"  Shape: {labels.shape}\")\n",
    "print(f\"  Columns: {list(labels.columns)}\")\n",
    "print(f\"\\n  First 5 rows:\")\n",
    "print(labels.head())\n",
    "print(f\"\\n  Data types:\")\n",
    "print(labels.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd51da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corpus combining transcripts with labels\n",
    "corpus_df, metadata = data_loader.create_corpus_with_labels()\n",
    "\n",
    "print(\"Combined Corpus Statistics:\")\n",
    "print(f\"  Total participants: {metadata['n_total_participants']}\")\n",
    "print(f\"  Depressed (PHQ_Binary=1): {metadata['n_depressed']}\")\n",
    "print(f\"  Non-Depressed (PHQ_Binary=0): {metadata['n_non_depressed']}\")\n",
    "print(f\"\\n  PHQ Score Statistics:\")\n",
    "print(f\"    Mean: {metadata['mean_phq_score']:.2f}\")\n",
    "print(f\"    Min: {metadata['min_phq_score']}\")\n",
    "print(f\"    Max: {metadata['max_phq_score']}\")\n",
    "\n",
    "print(f\"\\n  Corpus Shape: {corpus_df.shape}\")\n",
    "print(f\"  Columns: {list(corpus_df.columns)}\")\n",
    "print(f\"\\n  First 3 records:\")\n",
    "for idx in range(min(3, len(corpus_df))):\n",
    "    row = corpus_df.iloc[idx]\n",
    "    text_preview = row['text'][:80] + \"...\"\n",
    "    print(f\"\\n  Participant {row['Participant_ID']}: PHQ={row['PHQ_Score']}, Binary={row['PHQ_Binary']}\")\n",
    "    print(f\"    Text: {text_preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc098540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize depression distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Continuous PHQ scores\n",
    "axes[0].hist(corpus_df['PHQ_Score'], bins=15, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('PHQ Score')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of PHQ Scores (Continuous)')\n",
    "axes[0].axvline(corpus_df['PHQ_Score'].mean(), color='red', linestyle='--', \n",
    "               label=f'Mean: {corpus_df[\"PHQ_Score\"].mean():.1f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Binary distribution\n",
    "binary_counts = corpus_df['PHQ_Binary'].value_counts()\n",
    "colors = ['lightgreen', 'lightcoral']\n",
    "bars = axes[1].bar(['Non-Depressed (0)', 'Depressed (1)'], \n",
    "                    [binary_counts[0], binary_counts[1]], \n",
    "                    color=colors)\n",
    "axes[1].set_ylabel('Number of Participants')\n",
    "axes[1].set_title('Binary Depression Classification')\n",
    "\n",
    "# Add percentages\n",
    "for bar, count in zip(bars, [binary_counts[0], binary_counts[1]]):\n",
    "    height = bar.get_height()\n",
    "    pct = 100 * count / len(corpus_df)\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(count)}\\n({pct:.1f}%)',\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"  Non-Depressed: {binary_counts[0]} ({100*binary_counts[0]/len(corpus_df):.1f}%)\")\n",
    "print(f\"  Depressed: {binary_counts[1]} ({100*binary_counts[1]/len(corpus_df):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0c34f8",
   "metadata": {},
   "source": [
    "## Section 2: Text Preprocessing and Cleaning\n",
    "\n",
    "Now let's clean and preprocess the text data. We'll:\n",
    "- Convert to lowercase\n",
    "- Remove special characters and punctuation\n",
    "- Remove common stopwords\n",
    "- Tokenize into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5454ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor(remove_stopwords=config.REMOVE_STOPWORDS, \n",
    "                                lemmatize=config.LEMMATIZE)\n",
    "\n",
    "# Show example of preprocessing\n",
    "sample_text = corpus_df.iloc[0]['text']\n",
    "print(\"PREPROCESSING EXAMPLE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nOriginal text:\\n  {sample_text[:150]}...\")\n",
    "\n",
    "cleaned = preprocessor.clean_text(sample_text)\n",
    "print(f\"\\nAfter cleaning:\\n  {cleaned[:150]}...\")\n",
    "\n",
    "tokens = preprocessor.process(sample_text)\n",
    "print(f\"\\nTokenized and processed (first 20 tokens):\\n  {tokens[:20]}\")\n",
    "print(f\"\\nTotal tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2bfa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all texts\n",
    "print(f\"\\nProcessing {len(corpus_df)} texts...\")\n",
    "processed_tokens = preprocessor.process_batch(corpus_df['text'].values)\n",
    "\n",
    "# Calculate statistics\n",
    "token_counts = [len(tokens) for tokens in processed_tokens]\n",
    "print(f\"\\n✓ Preprocessing complete!\")\n",
    "print(f\"\\nStatistics after preprocessing:\")\n",
    "print(f\"  Total documents: {len(processed_tokens)}\")\n",
    "print(f\"  Average tokens per document: {np.mean(token_counts):.1f}\")\n",
    "print(f\"  Median tokens per document: {np.median(token_counts):.1f}\")\n",
    "print(f\"  Max tokens in a document: {np.max(token_counts)}\")\n",
    "print(f\"  Min tokens in a document: {np.min(token_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0014c39",
   "metadata": {},
   "source": [
    "## Section 3: Word Frequency Analysis (Starting Point)\n",
    "\n",
    "This is the simplest and most recommended starting point. We'll analyze:\n",
    "1. Most frequent words across all participants\n",
    "2. Word frequency distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda9783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize frequency analyzer\n",
    "freq_analyzer = WordFrequencyAnalyzer(config.MIN_WORD_FREQUENCY)\n",
    "\n",
    "# Compute word frequencies\n",
    "word_freq = freq_analyzer.compute_frequencies(processed_tokens)\n",
    "\n",
    "print(\"WORD FREQUENCY ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal unique words: {len(word_freq)}\")\n",
    "print(f\"Total word occurrences: {sum(word_freq.values())}\")\n",
    "\n",
    "# Get top words\n",
    "top_words = freq_analyzer.get_top_words(n=20)\n",
    "print(f\"\\nTop 20 Most Frequent Words:\")\n",
    "for i, (word, freq) in enumerate(top_words, 1):\n",
    "    print(f\"  {i:2d}. {word:15s} - {freq:4d} occurrences\")\n",
    "\n",
    "# Show word frequency distribution\n",
    "DataVisualizer.plot_top_words(top_words, title=\"Top 20 Most Frequent Words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3422baea",
   "metadata": {},
   "source": [
    "## Section 4: Correlate Word Frequencies with Depression Levels\n",
    "\n",
    "Now let's find words that are associated with depression. We'll compute correlations between word frequencies and PHQ scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd8b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze frequency by depression status\n",
    "phq_binary = corpus_df['PHQ_Binary'].values\n",
    "\n",
    "# Compute frequencies by group\n",
    "freq_analyzer.compute_frequencies_by_group(processed_tokens, phq_binary)\n",
    "\n",
    "print(\"WORD FREQUENCY BY DEPRESSION STATUS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nTop 10 words in NON-DEPRESSED group (PHQ_Binary=0):\")\n",
    "top_non_dep = freq_analyzer.get_top_words(n=10, group=0)\n",
    "for i, (word, freq) in enumerate(top_non_dep, 1):\n",
    "    print(f\"  {i:2d}. {word:15s} - {freq:4d}\")\n",
    "\n",
    "print(\"\\nTop 10 words in DEPRESSED group (PHQ_Binary=1):\")\n",
    "top_dep = freq_analyzer.get_top_words(n=10, group=1)\n",
    "for i, (word, freq) in enumerate(top_dep, 1):\n",
    "    print(f\"  {i:2d}. {word:15s} - {freq:4d}\")\n",
    "\n",
    "# Compare top words between groups\n",
    "top_all = freq_analyzer.get_top_words(n=15)\n",
    "words = [w for w, _ in top_all]\n",
    "freq_group0 = [freq_analyzer.word_freq_by_group[0].get(w, 0) for w in words]\n",
    "freq_group1 = [freq_analyzer.word_freq_by_group[1].get(w, 0) for w in words]\n",
    "\n",
    "DataVisualizer.plot_word_frequency_comparison(\n",
    "    words, freq_group0, freq_group1,\n",
    "    group1_label=\"Non-Depressed (PHQ=0)\",\n",
    "    group2_label=\"Depressed (PHQ=1)\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb00e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistical correlations\n",
    "phq_scores = corpus_df['PHQ_Score'].values\n",
    "\n",
    "corr_analyzer = CorrelationAnalyzer(processed_tokens, phq_scores)\n",
    "corr_analyzer.build_frequency_matrix(config.MIN_WORD_FREQUENCY)\n",
    "corr_analyzer.compute_correlations(method=config.CORRELATION_METHOD, phq_binary=phq_binary)\n",
    "\n",
    "print(\"\\nCORRELATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Method: {config.CORRELATION_METHOD}\")\n",
    "print(f\"Vocabulary size: {corr_analyzer.word_freq_matrix.shape[1]} words\")\n",
    "\n",
    "# Get top correlated words\n",
    "top_positive = corr_analyzer.get_top_correlated_words(n=15, positive=True)\n",
    "top_negative = corr_analyzer.get_top_correlated_words(n=15, positive=False)\n",
    "\n",
    "print(f\"\\nTop 15 words POSITIVELY correlated with depression:\")\n",
    "print(\"(Higher frequency = more likely to be depressed)\")\n",
    "for i, (word, corr) in enumerate(top_positive, 1):\n",
    "    print(f\"  {i:2d}. {word:15s} - correlation: {corr:+.4f}\")\n",
    "\n",
    "print(f\"\\nTop 15 words NEGATIVELY correlated with depression:\")\n",
    "print(\"(Higher frequency = less likely to be depressed)\")\n",
    "for i, (word, corr) in enumerate(top_negative, 1):\n",
    "    print(f\"  {i:2d}. {word:15s} - correlation: {corr:+.4f}\")\n",
    "\n",
    "# Visualize correlations\n",
    "all_words = [w for w, _ in top_positive + top_negative]\n",
    "all_corr = [c for _, c in top_positive + top_negative]\n",
    "\n",
    "DataVisualizer.plot_correlations(all_words, all_corr, \n",
    "                                title=f\"Word-Depression Correlations ({config.CORRELATION_METHOD})\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4c54f0",
   "metadata": {},
   "source": [
    "## Section 5: Visualize Frequency Distributions\n",
    "\n",
    "Let's create additional visualizations to better understand the data patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c36019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length distribution\n",
    "text_lengths = [len(tokens) for tokens in processed_tokens]\n",
    "\n",
    "DataVisualizer.plot_text_length_distribution(text_lengths, phq_binary)\n",
    "plt.show()\n",
    "\n",
    "print(\"Text Length Statistics:\")\n",
    "print(f\"  Mean: {np.mean(text_lengths):.1f} words\")\n",
    "print(f\"  Median: {np.median(text_lengths):.1f} words\")\n",
    "print(f\"  Std Dev: {np.std(text_lengths):.1f} words\")\n",
    "print(f\"  Range: {np.min(text_lengths)} - {np.max(text_lengths)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a80263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency distribution (log scale)\n",
    "freq_values = sorted(word_freq.values(), reverse=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# Linear scale\n",
    "axes[0].hist(freq_values, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Word Frequency')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Word Frequency Distribution (Linear Scale)')\n",
    "axes[0].axvline(np.mean(freq_values), color='red', linestyle='--', \n",
    "               label=f'Mean: {np.mean(freq_values):.1f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Log scale\n",
    "axes[1].hist(freq_values, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Word Frequency')\n",
    "axes[1].set_ylabel('Count (log scale)')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].set_title('Word Frequency Distribution (Log Scale)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Word Frequency Statistics:\")\n",
    "print(f\"  Mean frequency: {np.mean(freq_values):.2f}\")\n",
    "print(f\"  Median frequency: {np.median(freq_values):.2f}\")\n",
    "print(f\"  Max frequency: {np.max(freq_values)}\")\n",
    "print(f\"  Min frequency: {np.min(freq_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfc7eb5",
   "metadata": {},
   "source": [
    "## Section 6: Advanced Text Processing Approaches (Foundation for Future Work)\n",
    "\n",
    "Beyond simple word frequency, here are more advanced techniques you can explore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcba78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Analysis (Term Frequency - Inverse Document Frequency)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(\"ADVANCED TECHNIQUES PREVIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare texts\n",
    "texts = corpus_df['text'].values\n",
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=50, min_df=2, max_df=0.8)\n",
    "tfidf_matrix = tfidf.fit_transform(texts)\n",
    "\n",
    "print(f\"\\n1. TF-IDF Analysis\")\n",
    "print(f\"   - Considers both word frequency AND rarity across documents\")\n",
    "print(f\"   - Matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"   - Top TF-IDF terms: {tfidf.get_feature_names_out()[:20].tolist()}\")\n",
    "\n",
    "# TF-IDF importance for each class\n",
    "print(f\"\\n   Top TF-IDF features by depression status:\")\n",
    "tfidf_depressed = tfidf_matrix[phq_binary == 1].mean(axis=0).A1\n",
    "tfidf_non_depressed = tfidf_matrix[phq_binary == 0].mean(axis=0).A1\n",
    "\n",
    "top_dep_tfidf = np.argsort(tfidf_depressed)[-5:][::-1]\n",
    "top_non_dep_tfidf = np.argsort(tfidf_non_depressed)[-5:][::-1]\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(f\"   Depressed: {[feature_names[i] for i in top_dep_tfidf]}\")\n",
    "print(f\"   Non-Depressed: {[feature_names[i] for i in top_non_dep_tfidf]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f4228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-grams Analysis\n",
    "from collections import Counter\n",
    "\n",
    "def get_ngrams(tokens, n=2):\n",
    "    \"\"\"Extract n-grams from tokens\"\"\"\n",
    "    return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "print(f\"\\n2. N-grams Analysis\")\n",
    "print(f\"   - Captures word sequences and context\")\n",
    "print(f\"   - Examples: bigrams (2 words), trigrams (3 words), etc.\")\n",
    "\n",
    "# Extract bigrams from depressed group\n",
    "depressed_bigrams = []\n",
    "for idx, binary in enumerate(phq_binary):\n",
    "    if binary == 1:\n",
    "        bigrams = get_ngrams(processed_tokens[idx], n=2)\n",
    "        depressed_bigrams.extend(bigrams)\n",
    "\n",
    "depressed_bigram_freq = Counter(depressed_bigrams)\n",
    "print(f\"\\n   Top bigrams in DEPRESSED group:\")\n",
    "for bigram, freq in depressed_bigram_freq.most_common(5):\n",
    "    print(f\"     '{bigram}' - {freq} occurrences\")\n",
    "\n",
    "# Extract bigrams from non-depressed group\n",
    "non_depressed_bigrams = []\n",
    "for idx, binary in enumerate(phq_binary):\n",
    "    if binary == 0:\n",
    "        bigrams = get_ngrams(processed_tokens[idx], n=2)\n",
    "        non_depressed_bigrams.extend(bigrams)\n",
    "\n",
    "non_depressed_bigram_freq = Counter(non_depressed_bigrams)\n",
    "print(f\"\\n   Top bigrams in NON-DEPRESSED group:\")\n",
    "for bigram, freq in non_depressed_bigram_freq.most_common(5):\n",
    "    print(f\"     '{bigram}' - {freq} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05755d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key Findings Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY FINDINGS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1. DATASET OVERVIEW:\")\n",
    "print(f\"   - Total participants: {len(corpus_df)}\")\n",
    "print(f\"   - Depressed: {(phq_binary==1).sum()} ({100*(phq_binary==1).sum()/len(corpus_df):.1f}%)\")\n",
    "print(f\"   - Non-depressed: {(phq_binary==0).sum()} ({100*(phq_binary==0).sum()/len(corpus_df):.1f}%)\")\n",
    "\n",
    "print(f\"\\n2. WORD FREQUENCY INSIGHTS:\")\n",
    "print(f\"   - Total unique words: {len(word_freq)}\")\n",
    "print(f\"   - Most common word: '{top_words[0][0]}' ({top_words[0][1]} occurrences)\")\n",
    "print(f\"   - Average word frequency: {np.mean(list(word_freq.values())):.2f}\")\n",
    "\n",
    "print(f\"\\n3. DEPRESSION-RELATED PATTERNS:\")\n",
    "if top_positive:\n",
    "    print(f\"   - Words associated with depression:\")\n",
    "    for word, corr in top_positive[:3]:\n",
    "        print(f\"     • '{word}' (correlation: {corr:+.4f})\")\n",
    "\n",
    "print(f\"\\n4. TEXT CHARACTERISTICS:\")\n",
    "print(f\"   - Average text length: {np.mean(text_lengths):.0f} words\")\n",
    "print(f\"   - Texts are roughly similar length across groups\")\n",
    "\n",
    "print(f\"\\n5. RECOMMENDATIONS FOR NEXT STEPS:\")\n",
    "print(f\"   ✓ Current analysis: Simple word frequencies & correlations\")\n",
    "print(f\"   → Next steps:\")\n",
    "print(f\"      1. Explore TF-IDF for importance weighting\")\n",
    "print(f\"      2. Analyze n-grams for contextual patterns\")\n",
    "print(f\"      3. Apply machine learning (classification models)\")\n",
    "print(f\"      4. Investigate semantic patterns (word embeddings)\")\n",
    "print(f\"      5. Analyze linguistic features (sentiment, pronouns, etc.)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
